\documentclass{VUMIFPSbakalaurinis}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{caption}
\usepackage{color}
\usepackage{float}
\usepackage{graphicx}
\usepackage{listings}
\usepackage{subfig}
\usepackage{wrapfig}
\usepackage{makecell}
\usepackage{forest}
\usepackage{pgfplots}

\usepackage{enumitem}
\setlist{noitemsep, topsep=0pt}

\renewcommand\theadfont{\bfseries}
\newcommand\tab[1][1cm]{\hspace*{#1}}


\definecolor{myyellow}{RGB}{154, 125, 10}
\definecolor{myred}{RGB}{199, 0, 57}
\definecolor{myblue}{RGB}{52, 152, 219}
\definecolor{mygreen}{RGB}{30, 132, 73}


% Titulinio aprašas
\university{Vilniaus universitetas}
\faculty{Matematikos ir informatikos fakultetas}
\department{Programų sistemų katedra}
\papertype{Bakalauro darbas}
\title{Gestų kalbos vienetų atpažinimas iš video srauto}
\titleineng{Recognition of Sign language units from a video stream}
\author{Pranciškus Ambrazas}
\supervisor{j. asist. Linas Petkevičius}
\reviewer{dr. Vytautas Valaitis}
\date{Vilnius – \the\year}

% Nustatymai
% \setmainfont{Palemonas}   % Pakeisti teksto šriftą į Palemonas (turi būti įdiegtas sistemoje)
\bibliography{bibliografija}

\begin{document}
\maketitle

%% Padėkų skyrius
% \sectionnonumnocontent{}
% \vspace{7cm}
% \begin{center}
%     Padėkos asmenims ir/ar organizacijoms
% \end{center}

\sectionnonumnocontent{Santrauka}
Glaustai aprašomas darbo turinys: pristatoma nagrinėta problema ir padarytos
išvados. Santraukos apimtis ne didesnė nei 0,5 puslapio. Santraukų gale
nurodomi darbo raktiniai žodžiai. 
% Nurodomi iki 5 svarbiausių temos raktinių žodžių (terminų).
% Vienas terminas gali susidėti iš kelių žodžių.
\raktiniaizodziai{neuroniniai tinklai, konvoliuciniai neuroniniai tinklai, rekurentiniai neuroniniai tinklai, apsimokančios sistemos, gestų kalba, lietuvių gestų kalba}   

\sectionnonumnocontent{Summary}
Santrauka anglų kalba. Santraukos apimtis ne didesnė nei 0,5 puslapio.
\keywords{neural networks, convolutional neural networks, recurrent neural networks, machine learning, sign language, lithuanian sign language}

\tableofcontents

\sectionnonum{Įvadas}
Pasaulyje yra virš 7 milijardų žmonių, kurie kasdien tarpusavyje komunikuoja. Netgi 5\% visos žmonijos populiacijos sudaro žmonės, turintys klausos problemų. Vien 34 milijonai iš jų yra vaikai, iš kurių net 60\% praradusių klausą vaikystėje galėjo būti girdintys dabar, jei būtų imtąsi atitinkamų prevencinių priemonių. Paskaičiuota, kad iki 2050 metų žmonių, turinčių šias problemas, skaičius išaugs netgi iki 900 milijonų, o vien šiuo metu 1,1 milijardo jaunų žmonių nuo 11 iki 35 metų amžiaus yra ant klausos praradimo ribos dėl per didelio triukšmo \cite{WhoInt}.

\subsectionnonum{Gestų kalba}
Gestų kalba – tai geriausias būdas klausos negalią turintiems žmonėms bendrauti tarpusavyje. Ja pasaulyje bendrauja didžioji dalis klausos sutrikimus turinčiųjų, o amerikiečių gestų kalba (\textit{angl. American Sign Language (toliau - ASL)}) yra trečia pagal populiarumą Amerikoje ne anglų po ispanų ir kinų kalbų ir ketvirta apskirtai, kuria kalba virš 500 tūkstančių žmonių \cite{Gall}. 

Kiekviena šalis turi savo valstybinę kalbą - lietuvių, anglų, ispanų, rusų ar kitą. Lygiai taip pat kiekviena šalis turi ir savo gestų kalbą. Yra tokios kalbos kaip jau minėta amerikiečių, lietuvių (\textit{toliau - LGK})), argentiniečių ir kitos gestų kalbos. Netgi tam tikri šalių regionai turi specifinius tos pačios kalbos dialektus, kaip, tarkime, vien Lietuvoje yra žodinės kalbos aukštaičių, žemaičių, suvalkiečių ar dzūkų tarmės. 

Kiekviena gestų kalba turi skirtingą gramatiką ir sintaksę. Skirtingos gestų kalbos skiriasi tiek abėcėlėmis tiek pačiais gestais, dėl to skiriasi ir gramatika. Taip yra dėl to, kad nėra bendrinės gestų kalbos - vien Amerikoje yra virš 35 skirtingų gestų kalbų.

Vienas gestas gali turėti kelias prasmes. Kaip ir lietuvių kalboje žodis „kasa“ turi tris skirtingas reikšmes, taip ir gestų kalboje vienas gestas gali turėti keletą reikšmių. Tačiau iš kitos pusės gestas, parodytas truputėlį kitaip gali turėti visiškai priešingą reikšmę. Tarkime, ASL gestai „geras“ ir „blogas“ skiriasi tik puse į kurią atsuktas delnas, tačiau daugiau neturi jokių skirtumų.

\subsectionnonum{Gestų kalbos specifika}
Kiekviena gestų kalba susideda iš \textbf{trijų} pagrindinių dalių:
\begin{enumerate}
	\item\textbf{Statinė gestų kalba} - dar kitaip vadinama \textit{pirštų kalba} (\textit{angl. fingerspelling}). Tai įvairūs gestai rodomi vienos (ASL, LGK) ar net ir dviejų (britų ar vokiečių gestų kalba) rankų pagalba. Tai dažniausiai statiniai gestai, rodantys vieną raidę (\textit{žr. \ref{img:lgk} pav.}) ar net vieną žodį, kaip, pavyzdžiui, ASL „\textit{I love you}“\footnote{liet. Aš tave myliu} gestas. Yra galimybė žodžius išreikšti ir abėcėliškai. Lygiai taip pat žmonės kasdieninėje kalboje turi galimybę pasakyti paraidžiui. Tačiau yra įprasta jungti raides į žodžius. O žodžius galiausiai į sakinius. Vienas iš variantų, kuomet naudojama gestų kalba paraidžiui tai vardų pasakyme. Tačiau svarbu paminėti tai, kad dažniausiai gestakalbiai prisistatydami parodo gestą, kuris priklauso tik jiems. Tai tarsi parašas tam, kad nebereikėtų kreipiantis ar apibūdinant žmogų jo vardo sakyti paraidžiui. Toks gestas nebūtinai turi būti statinis.
	
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{img/lgk}
    \caption{Amerikiečių gestų kalbos abėcėlė}
    \label{img:lgk}
\end{figure}

	\item\textbf{Dinaminė gestų kalba} - tai žodžių lygio gestų kalba. Nesunku pastebėti, kad \ref{img:lgk} paveikslėlyje yra „Ą“, „D“, „Į“, „J“, „K“, „Ų“, „Z“ ar „Ž“ raidės, kurios priskiriamos dinaminių judesių klasei. Kaip ir yra žodžių, kurie priskiriami statinei gestų kalbai dėl savo kilmės, taip ir yra raidžių, kurios priskiriamos dinaminei gestų kalbai. Dinaminiais judesiais yra išreiškiami įvairūs gestų kalbos žodžiai tokie, kaip, pavyzdžiui, LGK yra „labas“, „mano“ ar „vardas“.
	
	\item\textbf{Kitos ypatybės} - emocijos veide, liežuvis, burna ir kūno laikysena. Tai taip pat labai svarbios gestų kalbos ypatybės. Pavyzdžiui, klausiant gestų kalba klausimo, jei bus pakelti antakiai, tai reikš, kad laukiamas ataskymas „taip“ arba „ne“. Tačiau, jei antakiai bus suraukti, tai reikš, kad klausiama su paaiškinimu „kas“, „kur“, „kaip“, „ką“.
\end{enumerate}


\subsectionnonum{Darbo tikslas}
Išanalizuoti gestų kalbos vienetų atpažinimo galimybes iš video srauto.

\subsectionnonum{Darbo uždaviniai}
\begin{itemize}
	\item Gestų kalbos video srautų paieška ir rengimas bei mokomosios medžiagos neuroniniams tinklams surinkimas;
	\item Susipažinimas su rekurentiniais neuroniniais tinklais;
	\item Gestų kalbos vienetų atpažinimas iš video srauto, pasinaudojant rekurentiniais neuroniniais tinklais.
\end{itemize}

\subsectionnonum{Darbo eiga}
\begin{itemize}
	\item Panašių ir jau įgyvendintų projektų paieška;
	\item Esamos sistemos patobulinimai;
	\item Rezultatų palyginimai.
\end{itemize}

\subsectionnonum{Panaudotos priemonės}
\begin{itemize}
	\item Python – programavimo kalba;
	\item TensorFlow – skirta darbui su apsimokančiomis sistemomis (\textit{angl. machine learning})
	\item Inception v3 – konvoliucinių neuroninių tinklų modelis;
	\item LSTM – rekurentinių neuroninių tinklų metodas;
	\item OpenCV – Python įrankis darbui su vaizdais;
\end{itemize}


\section{Apsimokančios sistemos}
\textbf{Apsimokančios sistemos} (\textit{angl. machine learning}) – metodų rinkinys, kuris sugeba automatiškai analizuoti duomenų struktūras ir tuomet apdoroti nematytus modelius, kad prognozuotų duomenis arba priimtų sprendimus kitais būdais \cite{doi:10.1080/09332480.2014.914768}.

\subsection{Prižiūrimas mokymas}
\textbf{Prižiūrimas mokymas} (\textit{angl. supervised learning}) - tai apsimokančių sistemų apmokymo būdas, kuomet duomenys mokymui yra paruošiami taip, kad kiekvienas duomuo turėtų ir atitinkamą rezultatą. Kitaip tariant, jei yra duomuo $a$, tai yra ir jį atitinkantis rezultatas, arba dar vadinama etiketė (\textit{angl. label}) $b$. Tai būdas, kuris veikia medžio principu.

\begin{table}[H]\footnotesize
  \centering
  \caption{Pavyzdinis prižiūrimo mokymo apmokymui paruoštų duomenų rinkinys}
  {\begin{tabular}{| c | c | c | c | c | c || c |} \hline
    \thead{Nr.} & \thead{Pirštas nr. 1} & \thead{Pirštas nr. 2} & \thead{Pirštas nr. 3} & \thead{Pirštas nr. 4} & \thead{Pirštas nr. 5} & \thead{Raidė} \\
    \hline
    1. & Atlenktas & Užlenktas & Užlenktas & Užlenktas & Užlenktas & \thead{A} \\
    2. & Atlenktas & Atlenktas & Atlenktas & Atlenktas & Atlenktas & \thead{B} \\
    3. & Atlenktas & Sulenktas & Užlenktas & Užlenktas & Užlenktas & \thead{C} \\
    4. & Atlenktas & Sulenktas & Sulenktas & Užlenktas & Užlenktas & \thead{Č} \\
    5. & Sulenktas & Sulenktas & Sulenktas & Sulenktas & Sulenktas & \thead{E} \\
    6. & Atlenktas & Sulenktas & Sulenktas & Sulenktas & Sulenktas & \thead{F} \\
    \hline
  \end{tabular}}
  \label{tab:priziurimasPavyzdys}
\end{table}

\ref{tab:priziurimasPavyzdys} lentelėje pateikiamas pavyzdys su supaprastinta lietuvių gestų kalbos abėcėle. Lentelėje pateikiamos piršų padėtys, o pirštai numeruojami pagal \ref{appendix:pirstai} priede pateikiamą pirštų numeraciją. Kiekvieno piršto padėtis šiame pavyzdyje gali būti: \textit{atlenktas, sulenktas, užlenktas}. Ir kiekvienai padėčiai esant pateikiamas rezultatas, arba kitaip - etiketė, kokią raidę abėcėlėje atitinka pavaizduotos pirštų padėtys.


\begin{table}[H]\footnotesize
  \centering
  \caption{Pavyzdinė praktinė užduotis}
  {\begin{tabular}{| c | c | c | c | c | c || c |} \hline
    \thead{Nr.} & \thead{Pirštas nr. 1} & \thead{Pirštas nr. 2} & \thead{Pirštas nr. 3} & \thead{Pirštas nr. 4} & \thead{Pirštas nr. 5} & \thead{Raidė} \\
    \hline
    1. & Atlenktas & Sulenktas & Sulenktas & Sulenktas & Sulenktas & \thead{?} \\
    \hline
  \end{tabular}}
  \label{tab:priziurimasUzdavinys}
\end{table}

\ref{tab:priziurimasUzdavinys} lentelėje pateikiamas uždavinys, kuriame nurodoma ta pati informacija, kuri buvo pateikta \ref{tab:priziurimasPavyzdys} lentelėje. Tačiau rezultatas nėra pateiktas, o jis randamas medžio principu.


\begin{figure}[H]
    \centering
    
\begin{forest}
  for tree={
    fit=band,% spaces the tree out a little to avoid collisions
  }
  [\textit{Pirštas nr. 4}
    [Atlenktas [\textbf{B}]]
    [Užlenktas
      [\textit{Pirštas nr. 3}
      	[Sulenktas [\textbf{Č}]]
	[Užlenktas
	  [\textit{Pirštas nr. 2}
	    [Užlenktas [\textbf{A}]]
	    [Sulenktas [\textbf{C}]]
	  ]
	]
      ]
    ]
    [Sulenktas
      [\textit{Pirštas nr. 1}
      	[Sulenktas [\textbf{E}]]
      	[Atlenktas [\textbf{F}]]
      ]
    ]  
  ]
\end{forest}
    \caption{Galimybių medis}
    \label{img:medis}
\end{figure}


Vien iš šio medžio galimybių medžio galima matyti, kad pilnai užtenka sprendimui nusakyti 3 pirštų, kadangi rezultatų nėra daug. Jei būtų imama visa abėcėlės aibė, tuomet rezultato nustatymui būtų naudojama galimai visų pirštų padėtys.

\subsection{Neprižiūrimas mokymas}
\textbf{Neprižiūrimas mokymas} (\textit{angl. unsupervised learning}) - mokymas, kuomet duomenims nėra priskiriamos teisingos etiketės ar teisingi rezultatai. Pavyzdžiui, tai galėtų atitikti naujos kalbos mokymąsi be mokytojo ar bet kokio žodyno. Kuomet pastoviai matomas vis tas pats tekstas, žodžiai tampa atpažįstami, tačiau išversti jų neišeina. Tačiau tai nesukelia jokių nepatogumų, jei į tekstą reikia įrašyti tinkamą žodį, kuomet dėl daugybės duomenų yra aišku koks žodis su kokia galūne turėtų būti įrašytas.

\begin{table}[H]\footnotesize
  \centering
  \caption{Pavyzdinis neprižiūrimo mokymo apmokymui paruoštų duomenų rinkinys}
  {\begin{tabular}{| c | c | c | c | c | c |} \hline
    \thead{Nr.} & \thead{Pirštas nr. 1} & \thead{Pirštas nr. 2} & \thead{Pirštas nr. 3} & \thead{Pirštas nr. 4} & \thead{Pirštas nr. 5}  \\
    \hline
    1. & Atlenktas & Užlenktas & Užlenktas & Užlenktas & Užlenktas\\
    2. & Atlenktas & Atlenktas & Atlenktas & Atlenktas & Atlenktas \\
    3. & Atlenktas & Sulenktas & Užlenktas & Užlenktas & Užlenktas \\
    4. & Atlenktas & Sulenktas & Sulenktas & Užlenktas & Užlenktas \\
    5. & Sulenktas & Sulenktas & Sulenktas & Sulenktas & Sulenktas \\
    6. & Atlenktas & Sulenktas & Sulenktas & Sulenktas & Sulenktas \\
    \hline
  \end{tabular}}
  \label{tab:nepriziurimasPavyzdys}
\end{table}

\ref{tab:nepriziurimasPavyzdys} lentelėje pateikiamas pavyzdinis neprižiūrimam mokymui apmokyti paruoštų duomenų rinkinys. Duomenys tokie patys, kaip ir \ref{tab:priziurimasPavyzdys} lentelėje, tačiau nėra teisingo atsakymo sulpelio \textbf{„Raidė“}. Apmokius tokią sistemą būtent tokiais duomenimis vienas iš tikėtinų scenarijų, kur galima būtų panaudoti tokią sistemą, tai nuspėti, kokios raidės yra labiausiai tikėtinos ar tiesiog numatyti, kokia labiausiai tikėtina raidžių seka bus rodoma.


\subsection{Praktinis mokymas}
\textbf{Praktinis mokymas} (\textit{angl. reinforcement learning}) - labiausiai dirbtinį intelektą atitinkančių apsimokančių sistemų apmokymo modelis. Šis mokymas pagrįstas praktiniais bandymais. Kiekvienas teisingai gautas rezultatas yra būdas, kuriuo reikėtų sekti, ir kiekvienas blogai gautas rezultatas, yra būdas, kurio vertėtų atsisakyti. Dažniausiai šis apmokymo būdas naudojamas sistemą apmokant žaisti žaidimus. Vienas iš labiausiai žinomų būtent šiuo apmokymo būdu apmokytų modelių yra \textit{AlphaZero}, kuris sugeba laimėti prieš pasaulio šachmatų čempionus. Tai puikus pavyzdys to, kaip kompiuteris iš laimėjimų, už kuriuos gauna taškus, ir pralaimėjimų, už kuriuos jam taškai atimami, sugeba rasti laimėjimo strategijas kiekviename žingsnyje ir taip, nuolatos tobulėdamas, laimėti dvikovas ar apskritai spręsti uždavinius, kuriuose reikalingas pastabumas ir strategijų kūrimas.


\section{Neuroniniai tinklai}
Žmogaus smegenys yra labai sudėtingas, nelijinis ir paralelinis kompiuteris \cite{Hay09}. Kiekvieno žmogaus kūnas yra sudarytas iš milijardų nervinių ląstelių vadinamų neuronais. Jie sukuria ir/arba perduoda elektrocheminius impulsus. Neuronai tarpusavyje yra sujungti dendritais, ant kurių yra sinapsės. 

Kiekvienas sužadintas neuronas dėl pasikeitusios temperatūros, spaudimo, skausmo ar kitų veiksnių, perduoda informaciją į smegenis dėl sprendimo, ką daryti, priėmimo. Tai, kaip ir buvo paminėta, neuronų paskirtis - siųsti signalą iš vieno neurono į kitą, kol galiausiai signalas pasiekia smegenis. Svarbu ir tai, kad kiekvienas neuronas yra nepriklausomas nuo kito. Tai tik grandis, kuri yra atsakinga už signalo priėmimą ir perdavimą. Smegenims gavus signalą, jį apdorojus ir priėmus sprendimą, signalas tuo pačiu keliu siunčiamas atgal, kol pirmąjį sužadinimą gavęs neuronas sulaukia atsakymo. 

\subsection{Perceptronas}

Parceptronas (\textit{angl. perceptron}) – kompiuterinis modelis, skirtas atkartoti žmogaus kūne esančių neuronų darbą. Toliau pateikiamas perceptrono pavyzdys.

\begin{figure}[H]
	\centering
	\tikzset{%
		every neuron/.style={
			circle,
			draw,
			minimum size=1cm
		},
	}
	
	\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
	\centering
	
	\foreach \m/\l [count=\y] in {1,2,3,4}
	\node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};
	
	\foreach \m [count=\y] in {1,2,3,4}
	\node [every neuron/.try, neuron \m/.try ] (weight-\m) at (2,2.5-\y) {};
	
	\foreach \m [count=\y] in {1}
	\node [every neuron/.try, neuron \m/.try ] (sum-\m) at (4,1.6-\y*1.6) {};
	
	\foreach \m [count=\y] in {1}
	\node [every neuron/.try, neuron \m/.try ] (funk-\m) at (6,1.6-\y*1.6) {};
	
	\foreach \m [count=\y] in {1}
	\node [every neuron/.try, neuron \m/.try ] (output-\m) at (8,1.6-\y*1.6) {};
	
	\foreach \i in {1,...,4}
	\draw [->] (input-\i) -- (weight-\i);
	
	\foreach \i in {1,...,4}
	\draw [->] (weight-\i) -- (sum-1);
	
	\draw [->] (sum-1) -- (funk-1);
	
	\draw [->] (funk-1) -- (output-1);
	
	\foreach \l [count=\x from 0] in {Įeigos\\, Svoriai\\, Suma\\, Aktyvacijos\\funkcija, Išeiga\\}
	\node [align=center, above] at (\x*2,2) {\l};
	
	
	\draw (0,1.49) node {$i_1$};
	\draw (0,0.49) node {$i_2$};
	\draw (0,0.045) node {$\vdots$};
	\draw (0,-0.51) node {$i_n$};
	\draw (0,-1.51) node {$1$};
	
	\draw (2,1.49) node {$w_1$};
	\draw (2,0.49) node {$w_2$};
	\draw (2,0.045) node {$\vdots$};
	\draw (2,-0.51) node {$w_n$};
	\draw (2,-1.51) node {$b$};
	
	\draw (4,0) node {$\sum$};
	
	\draw (6,0) node {$f(x)$};
	
	\draw (8,0) node {$o$};
	\end{tikzpicture}
	\caption{Perceptrono pavyzdys} \label{fig:perceptron}
\end{figure}

\ref{fig:perceptron} paveikslėlyje pavaizduotame pavyzdyje esančią išeigą galima aprašyti formule:
\begin{equation}
	o = f((\sum_{j=0}^{n}i_j \cdot w_j) + 1 \cdot b)
	\label{eq:perceptron}
\end{equation}

\ref{eq:perceptron} formulėje $i_j$ - $j$-toji įeiga, $w_j$ - $j$-tosios įeigos svorinis koeficientas, $b$ - poslinkio koeficientas, kuris dažniausiai kaip įeigos vertę turi $1$.

Kiekvienas perceptronas gali gauti vieną ar kelias įeigas (\textit{angl. input}). Visų šių įeigų svorių suma yra sudedama ir paskui apdorojama aktyvacijos funkcija. Pritaikius aktyvacijos funkciją yra gaunama išeiga (\textit{angl. output}). Yra keletas skirtingų aktyvacijos funkcijų. Pačios populiariausios pateikiamos \ref{fig:aktyvacijosfunkc} diagramoje.
\begin{figure}[H]
	\centering
	
	\begin{tikzpicture}
	\begin{axis}[
	axis lines = left,
	width=10cm,
	xlabel = $x$,
	ylabel = {$f(x)$},
	ymax = 2,
	ymajorgrids=true,
	xmajorgrids=true,
	legend pos=north west,
	grid style=dashed,
	]
	%Below the red parabola is defined
	\addplot [
	domain=-6:6, 
	samples=100, 
	color=red,
	]
	{0.05*x};
	\addlegendentry{tiesinė}
	%Here the blue parabloa is defined
	\addplot [
	domain=-6:6,
	samples=100, 
	color=blue,
	]
	{1/(1+exp(-x))};
	\addlegendentry{logistinė regresija}
	
	\addplot [
	domain=-6:6,
	samples=100, 
	color=green,
	]
	{tanh(x)};
	\addlegendentry{tanh}
	
	
	\addplot [
	domain=0:6,
	samples=100, 
	color=black,
	]
	{x};
	\addlegendentry{ReLU}
	
	
	\addplot [
	domain=-6:0,
	samples=100, 
	color=black,
	]
	{0};
	
	\end{axis}
	\end{tikzpicture}
	
	\caption{Aktyvacijos funkcijos} \label{fig:aktyvacijosfunkc}
\end{figure}


\ref{fig:aktyvacijosfunkc} diagramoje pateikiamos šios funkcijos:

\begin{itemize}
	\item Tiesinė – $f(x) = a \cdot x$;
	\item Logistinės regresijos – $f(x) = \frac{1}{1+e^{-x}}$;
	\item Tanh – $f(x) = \tanh(x) = \frac{2}{1+e^{-2x}} - 1$;
	\item ReLU – $f(x) = \begin{cases}
	0 & \text{, kai } x < 0 \\
	x & \text{, kai } x \ge 0
	\end{cases} $.
\end{itemize}

\subsection{Daugiasluosknis perceptronas}

\textbf{Daugiasluoksnis perceptronas} (\textit{angl. multilayer perceptron}) – strukūra, sudaryta iš kelių sluoksnių perceptronų. 

Dažniausiai daugiasluoksnis perceptronas turi tris ar daugiau sluoksnių – įeigos (\textit{input layer}), paslėptasis (\textit{hidden layer}) ir išeigos (\textit{output layer}) sluoksnius. Paslėptajame sluoksnyje gali būti daugiau nei vienas sluoksnis. Daugiasluoksnis perceptronas kaip aktyvacijos funkciją naudoja nelinijines aktyvacijos funkcijas. Dažniausiai tai būna \textit{tanh} ar loginės regresijos funkcijos. Kiekvienas sluoksnio elementas yra sujungtas su kito sluoksnio elementu, todėl tai sudaro pilnai apjungtą (\textit{angl. fully connected}) tinklą. Yra pavyzdžių, kur daugiausluoksniai perceptronai naudojami atpažinti žodinę kalbą ar versti tekstus. 

Toliau pateikiamas daugiasluoksnio perceptrono pavyzdys \ref{fig:multilayer} paveikslėlyje.

\begin{figure}[H]
	\centering
	\tikzset{%
		every neuron/.style={
			circle,
			draw,
			minimum size=1cm
		},
	}
	
	\begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]
	\centering
	
	\foreach \m/\l [count=\y] in {1,2,3}
	\node [every neuron/.try, neuron \m/.try] (input-\m) at (0,1.9-\y) {};
	
	\foreach \m [count=\y] in {1,2,3,4}
	\node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y) {};
	
	\foreach \m [count=\y] in {1,2}
	\node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,2.3-\y*1.6) {};
	
	\foreach \i in {1,...,3}
	\foreach \j in {1,...,4}
	\draw [->] (input-\i) -- (hidden-\j);
	
	\foreach \i in {1,...,4}
	\foreach \j in {1,...,2}
	\draw [->] (hidden-\i) -- (output-\j);
	
	\foreach \l [count=\x from 0] in {Įeigos, Paslėptasis, Išeigos}
	\node [align=center, above] at (\x*2,2) {\l \\ sluoksnis};
	\end{tikzpicture}
	\caption{Daugiasluoksnio perceptrono pavyzdys} \label{fig:multilayer}
\end{figure}



\subsection{Dirbtiniai neuroniniai tinklai}
\textbf{Dirbtiniai neuroniniai tinklai} (\textit{angl. artificial neural networks}) – struktūra, sukurta remiantis žmogaus nervinės sistemos darbu. Dirbtiniai neuroniniai tinklai gali būti išmokinti atlikti klasifikavimo, spėjimo, sprendimų priėmimo ir kitas užduotis.


Dirbtiniai neuroniniai tinklai remiasi daugiasluoksnio perceptrono principu ir susideda iš šių sluoksnių - įeigos, paslėptojo, kuris gali būti sudarytas iš kelių sluoksnių, ir išeigos.






\subsection{Konvoliuciniai neuroniniai tinklai}

\textbf{Konvoliuciniai neuroniniai tinklai} (\textit{angl. convoliutional neural networks}) – specialios rūšies vienpusiai (\textit{angl. feed-forward}) neuroniniai tinklai, kurie remiasi daugiasluoksnio perceptrono principu. Šie tinklai, kurie remiasi \textit{ReLU} principu yra kelis kartus greitesni, nei tie, kurie remiasi kitais principais, pavyzdžiui, \textit{tanh} \cite{NIPS2012_4824}. Toliau aptariami keli pagrindiniai konvoliucinių neuroninių tinklų sluoksniai.

\subsubsection{Konvoliucinis sluoksnis}

\textbf{Konvoliucinis sluoksnis} (\textit{angl. convoliution layer}) – sluoksnis, skirtas išskirti savybes. Šio sluoksnio 
pritaikymą galima skaidyti į tokias operacijas:


\begin{enumerate}
	\item \textbf{Įeiga}, susidedanti iš $ W_1 \times H_1 \times D_1 $, kur $ W_1 $ - plotis, $ H_1 $ - aukštis ir $ D_1 $ - gylis;
	\item \textbf{Parametrai}, kurie susideda iš $ F $, $ K $, $ P $ ir $ S $, kur:
	\begin{itemize}
		\item $ F $ - filtro dydis (dažniausiai taikomas $ 3 \times 3 $ filtras);
		\item $ K $ - filtrų skaičius (dažniausiai naudojamas  $ 2^n $, kur $ n $ - natūralusis skaičius);
		\item $ P $ - papildomas rėmelis matricai, sudarytas iš 0. Dažniausiai naudojama $ M = \frac{F - 1}{2} $, kur $ M $ yra iš kiekvienos matricos pusės pridedamų eilučių ar stulpelių skaičius, sudarytas iš $0$, tam, kad matrica nepakeistų savo dydžio po šio sluoksnio pritaikymo; 
		\item $ S $ - žingsnis, per kiek paslenkamas filtras (dažniausiai naudojamas $ 1 $);
	\end{itemize}
	\item \textbf{Išeiga}, susidedanti iš $ W_2 \times H_2 \times D_2 $, kur $ W_2 = \frac{W_1 - F + 2P}{S} + 1 $ - plotis, $ H_2 = \frac{H_1 - F + 2P}{S} + 1 $ - aukštis ir $ D_2 = K $ - gylis
\end{enumerate}

\begin{table}[H]\footnotesize
	\centering
	\caption{Pavyzdinės konvoliucinio sluoksnio užduoties ypatybės}
	{\begin{tabular}{| c | c | c | c | c | c | c | c | c | c |} 
		\hline
		\multicolumn{3}{| c |}{\thead{Įeiga}} &
		\multicolumn{4}{ c |}{\thead{Parametrai}} & \multicolumn{3}{ c |}{\thead{Išeiga}} \\
		\hline
		$W_1$ & $H_1$ & $D_1$ & $F$ & $K$ & $P$ & $S$ & $W_2$ & $H_2$ & $D_2$  \\
		\hline
		3 & 3 & 1 & 3 $\times$ 3 & 1 & 1 & 1 & 3 & 3 & 1 \\
		\hline
	\end{tabular}}
	\label{tab:convT}
\end{table}
	
Toliau, \ref{eq:convl} formulėje pateikiamas pavyzdys, kuriame naudojamos \ref{tab:convT} lentelėje pateiktos pavyzdinės konvoliucinio sluoksnio užduoties ypatybės. Spalvos šioje formulėje žymi skirtingų matricų elementus, kur geltona - įeigos matricos elementų spalva, raudona - papildomo rėmelio $P$ spalva, mėlyna - filtro matricos spalva, o žalia - išeigos matricos elemento spalva. \ref{eq:convl1} ir \ref{eq:convl2} formulėse pateikiami konkretūs pavyzdžiai, kuriais remiantis buvo gautos \ref{eq:convl} formulės reikšmės.

\begin{equation}\label{eq:convl}
\begin{bmatrix}
\textcolor{myyellow}{1} & \textcolor{myyellow}{8} & \textcolor{myyellow}{6} \\
\textcolor{myyellow}{9} & \textcolor{myyellow}{2} & \textcolor{myyellow}{4} \\
\textcolor{myyellow}{3} & \textcolor{myyellow}{7} & \textcolor{myyellow}{5}
\end{bmatrix}
\cdot
\begin{bmatrix}
\textcolor{myblue}{1} & \textcolor{myblue}{0} & \textcolor{myblue}{1} \\
\textcolor{myblue}{0} & \textcolor{myblue}{1} & \textcolor{myblue}{0} \\
\textcolor{myblue}{1} & \textcolor{myblue}{0} & \textcolor{myblue}{1}
\end{bmatrix}
=
\begin{bmatrix}
\textcolor{mygreen}{3} & \textcolor{mygreen}{21} & 8 \\
24 & 17 & 19 \\
5 & 20 & 7
\end{bmatrix}
\end{equation}

\begin{equation}\label{eq:convl1}
\textcolor{myred}{0} \cdot \textcolor{myblue}{1}+\textcolor{myred}{0} \cdot \textcolor{myblue}{0}+\textcolor{myred}{0} \cdot \textcolor{myblue}{0}+\textcolor{myred}{0} \cdot \textcolor{myblue}{0}+\textcolor{myyellow}{1} \cdot \textcolor{myblue}{1}+\textcolor{myyellow}{8} \cdot \textcolor{myblue}{0}+\textcolor{myred}{0} \cdot \textcolor{myblue}{1}+\textcolor{myyellow}{9} \cdot \textcolor{myblue}{0}+\textcolor{myyellow}{2} \cdot \textcolor{myblue}{1}=\textcolor{mygreen}{3}
\end{equation}


\begin{equation}\label{eq:convl2}
\textcolor{myred}{0} \cdot \textcolor{myblue}{1}+\textcolor{myred}{0} \cdot \textcolor{myblue}{0}+\textcolor{myred}{0} \cdot \textcolor{myblue}{1}+\textcolor{myyellow}{1} \cdot \textcolor{myblue}{0}+\textcolor{myyellow}{8} \cdot \textcolor{myblue}{1}+\textcolor{myyellow}{6} \cdot \textcolor{myblue}{0}+\textcolor{myyellow}{9} \cdot \textcolor{myblue}{1}+\textcolor{myyellow}{2} \cdot \textcolor{myblue}{0}+\textcolor{myyellow}{4} \cdot \textcolor{myblue}{1}=\textcolor{mygreen}{21}
\end{equation}

\subsubsection{Telkimo sluoksnis}

\textbf{Telkimo sluoksnis} (\textit{angl. pooling layer}) – sluoksnis, skirtas sumažinti matricą, paliekant tik svarbiausias jos dalis. Dažniausiai naudojamos vidutinės (\textit{angl. average pooling}) arba didžiausios (\textit{angl. max pooling}) reikšmės operacijos. 

Telkimo sluoksnio operacijas galima skaidyti į tokias dalis:

\begin{enumerate}
	\item \textbf{Įeiga}, susidedanti iš $ W_1 \times H_1 \times D_1 $, kur $ W_1 $ - plotis, $ H_1 $ - aukštis ir $ D_1 $ - gylis
	\item \textbf{Parametrai}, kurie susideda iš $ F $ ir $ S $, kur $ F $ - filtro dydis (dažniausiai taikomas $ 2 \times 2 $ filtras) ir $ S $ - žingsnis, per kiek paslenkamas filtras (dažniausiai naudojamas $ 2 $)
	\item \textbf{Išeiga}, susidedanti iš $ W_2 \times H_2 \times D_2 $, kur $ W_2 = \frac{W_1 - F}{S} + 1 $ - plotis, $ H_2 = \frac{H_1 - F}{S} + 1 $ - aukštis ir $ D_2 = D_1 $ - gylis
\end{enumerate}


\begin{table}[H]\footnotesize
	\centering
	\caption{Pavyzdinės telkimo sluoksnio užduoties ypatybės}
	{\begin{tabular}{| c | c | c | c | c | c | c | c |} 
		\hline
		\multicolumn{3}{| c |}{\thead{Įeiga}} &
		\multicolumn{2}{ c |}{\thead{Parametrai}} & \multicolumn{3}{ c |}{\thead{Išeiga}} \\
		\hline
		$W_1$ & $H_1$ & $D_1$ & $F$ & $S$ & $W_2$ & $H_2$ & $D_2$  \\
		\hline
		4 & 4 & 1 & 2 $\times$ 2 & 2 & 3 & 3 & 1 \\
		\hline
	\end{tabular}}
	\label{tab:pollT}
\end{table}	
Toliau, \ref{eq:poll} formulėje pateikiamas pavyzdys, kuriame naudojamos \ref{tab:pollT} lentelėje pateiktos pavyzdinės telkimo sluoksnio užduoties ypatybės. Spalvos šioje formulėje žymi filtro su žingsniu pritaikytas operacijas gauti išeigai. \ref{eq:poll1} ir \ref{eq:poll2} formulėse pateikiami konkretūs pavyzdžiai, kuriais remiantis buvo gautos \ref{eq:poll} formulės reikšmės.

\begin{equation}\label{eq:poll}\def\y{\color{myyellow}}\def\b{\color{myblue}}\def\r{\color{myred}}\def\g{\color{mygreen}}
\begin{bmatrix}
\y{1} & \y{3} & \b{1} & \b{3} \\
\y{2} & \y{5} & \b{4} & \b{2} \\
\g{4} & \g{3} & \r{2} & \r{5} \\
\g{2} & \g{5} & \r{4} & \r{2} \\
\end{bmatrix}
= 
\begin{bmatrix}
\y{5} & \b{4} \\
\g{5} & \r{5} 
\end{bmatrix}
\end{equation}
\begin{equation}\label{eq:poll1}
max(
\begin{bmatrix}
\textcolor{myyellow}{1} & \textcolor{myyellow}{3} \\
\textcolor{myyellow}{2} & \textcolor{myyellow}{5}
\end{bmatrix}
) = \textcolor{myyellow}{5}
%1, 3, 1, 3, 2, 5, 2, 4, 3
\end{equation}
\begin{equation}\label{eq:poll2}
max(
\begin{bmatrix}
\textcolor{myblue}{1} & \textcolor{myblue}{3} \\
\textcolor{myblue}{4} & \textcolor{myblue}{2} 
\end{bmatrix}
) = \textcolor{myblue}{4}
\end{equation}

\subsubsection{Atsisakymo sluoksnis}
\textbf{Atsisakymo sluoksnis} (\textit{angl. dropout layer}) – konvoliucinių tinklų sluoksnis, skirtas normalizuoti ir sureguliuoti tarpusavyje susijusių neuronų sąryšius, skirtus perduoti signalus. Mokymo fazėje dažniausiai ištrinamos neuronuose esančios reikšmės tam, kad šis per naują apsimokytų. Galimai netgi atsisakoma tam tikrų neuronų darbo \cite{DBLP:journals/corr/abs-1207-0580}.

\begin{figure}[H]
	\centering
	
	\begin{minipage}{.4\textwidth}
		\centering
		\tikzset{%
			every neuron/.style={
				circle,
				draw,
				minimum size=0.5cm
			},
		}
		
		\begin{tikzpicture}[x=1cm, y=1cm, >=stealth]
		\centering
		
		\foreach \m/\l [count=\y] in {1,2,3}
		\node [every neuron/.try, neuron \m/.try] (input-\m) at (0,1.9-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,4}
		\node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2}
		\node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,2.3-\y*1.6) {};
		
		\foreach \i in {1,...,3}
		\foreach \j in {1,...,4}
		\draw [->] (input-\i) -- (hidden-\j);
		
		\foreach \i in {1,...,4}
		\foreach \j in {1,...,2}
		\draw [->] (hidden-\i) -- (output-\j);
		
		\end{tikzpicture}
		\caption{Standartinis neuroninis tinklas} \label{fig:snn}
	\end{minipage}
	\begin{minipage}{.4\textwidth}
		\centering
		\tikzset{%
			every neuron/.style={
				circle,
				draw,
				minimum size=0.5cm
			},
			cross/.style={cross out, draw=black, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt},
			%default radius will be 1pt. 
			cross/.default={0.19cm},
		}
		
		\begin{tikzpicture}[x=1cm, y=1cm, >=stealth]
		\centering
		
		\foreach \m/\l [count=\y] in {1,2,3}
		\node [every neuron/.try, neuron \m/.try] (input-\m) at (0,1.9-\y) {};
		
		\foreach \m [count=\y] in {1,2,3,4}
		\node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2.5-\y) {};
		
		\foreach \m [count=\y] in {1,2}
		\node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,2.3-\y*1.6) {};
		
		\foreach \i in {1,3}
		\foreach \j in {1,3}
		\draw [->] (input-\i) -- (hidden-\j);
		
		\foreach \i in {1,3}
		\foreach \j in {1,...,2}
		\draw [->] (hidden-\i) -- (output-\j);
		
		\draw (0,-0.1) node[cross] {};
		
		\draw (2,0.5) node[cross] {};
		
		\draw (2,-1.5) node[cross] {};
		\end{tikzpicture}
		\caption{Tinklas po atsisakymo sluoksnio} \label{fig:sdnn}
	\end{minipage}
\end{figure}


\subsection{Rekurentiniai neuroniniai tinklai}

\textbf{Rekurentiniai neuroniniai tinklai} (\textit{angl. recurrent neural networks}) – vienpusiai neuroniniai tinklai, kurie remiasi daugiasluoksnio perceptrono principu. Šie tinklai, apima kitų laiko vienetų apdorotą informaciją ir bendrą kitimą laike \cite{DBLP:journals/corr/Lipton15}.


\begin{figure}[H]
	\centering
	\includegraphics[scale=0.4]{img/rnn}
	\caption{Rekurentinių neuroninių tinklų veikimo principas}
	\label{img:rnn}
\end{figure}

\ref{img:rnn} paveiksėlyje yra pavaizduotas bendrinis rekurentinių neuroninių tinklų veikimo principas, kurį galima užrašyti formule:

\begin{equation}\label{eq:rnn}
h_t = f_w(h_{t-1}, x_t)
\end{equation}

Kur $h_t$ - paslėpto sluoksnio būsena laiko momentu $t$, kurią dar būtų galima vadinti $t$ žingsnio išeiga, $f_w$ - funkcija $f$ su parametrais $w$, $h_{t-1}$ - praėjusio žingsnio būsena, o $x_t$ - įeigos vektorius. Iš šios formulės galima pastebėti, kad kiekviena būsena gauna praeito žingsnio būsena, kuri yra reikalinga norint stebėti būsenas kintant laike	.


\subsubsection{Rekurentinių neuroninių tinklų tipai}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.3]{img/nn-tipai}
	\caption{Rekurentinių neuroninių tinklų tipai}
	\label{img:tipai}
\end{figure}

\ref{img:tipai} paveikslėlyje pavaizduoti keturi skirtingi būdai, kuriais naudojantis rekurentiniai neuroniniai tinklai veikia. Rausvos spalvos kvadratėlis reiškia įeigą, žalsvas - paslėptuosius sluoksnius, o melsvas - išeigą. Pateikiami šie būdai:

\begin{itemize}
	\item \textbf{Vienas su vienu} (\textit{angl. one to one}) – būdas, kuriame yra viena įeiga, paslėptasis sluoksnis ir išeiga. Šis būdas dažniausiai taikomas konstruojant konvoliucinius neuroninius tinklus. Kaip pavyzdį galima pateikti paveikslėlio atpažinimą. Tai galėtų būti statinės gestų kalbos atpažinimas. Dažnai naudojami KNN;
	\item \textbf{Vienas su daug} (\textit{angl. one to many}) – būdas, kuriame yra viena įeiga, bet kelios išeigos. Vienas iš panaudojimo būdų galėtų būti sakinio suformavimas iš paveikslėlio. Toks tinklas ne tik atpažįsta pagrindinį objektą kadre, bet ir apibūdina esančią aplinką, daro kitus sprendimus;
	\item \textbf{Daug su vienu} (\textit{angl. many to one}) – būdas, kuriame yra daug įeigų, bet tik viena išeiga. Tokio būdo pavyzdys galėtų būti vieno žodžio, tarkime, „labas“ atpažinimas iš video sraudo.
	\item \textbf{Daug su daug} (\textit{angl. many to many}) – būdas, kuriame yra daug įeigų ir daug išeigų. Šis būdas gali būti skaidomas į dvi dalis:
	\begin{itemize}
		\item \textbf{Priklausomas} - įeigų skaičius sutampa su išeigų skaičiumi. Kiekviena įeiga turi savo išeigos atitikmenį. Tai būtų dalinai galima gretitinti su \textit{vienas su vienu} būdu. Pavyzdys šios atšakos galėtų būti video srauto klasifikacija pagal kiekvieną kadrą - nuolatinis atnaujinimas, to kas galėjo būti pasakyta, pavyzdžiui, gestų kalboje.
		\item \textbf{Nepriklausomas} - įeigos skaičius galimai nesutampa su išeigų skaičiumi. Kiekviena įeiga yra nepriklausoma ir išeigos dėliojamos pagal tam tikrus aspektus. Tokio būdo pavyzdys galėtų būti neuroniniai tinklai, kurie atlieka vertėjo funkcijas, pavyzdžiui, iš anglų į lietuvių kalbas, nes skiriasi tiek gramatika, tiek sakinių stilistika.
	\end{itemize}
\end{itemize}

\subsubsection{Rekurentinių neuroninių tinklų modeliai}

Viena pagrindinių problemų, su kuria susiduria paprastieji rekurentiniai neuroniniai tinklai yra nykstančių gradientų problema (\textit{angl. vanishing gradient problem}). Tai problema, kurios metu kiekvieno laiko momentu perceptronas apskaičiuoja naujas reiškmes iš praeitame žingsnyje turimų duomenų ir kaip įeiga priima praeito laiko momento išeigą. 

\begin{equation}
	f(w_n \cdot h_n)=h_{n+1}
\end{equation}

Šioje formulėje $w_n$ - $n$-tojo sluoksnio svoris, $h_n$ - $n$-tojo sluoksnio paslėptoji būsena, o $f(x)$ - aktyvacijos funkcija.

Tinklo pabaigoje gaunamas praradimas (\textit{angl. loss}) arba kitaip - skirtumas tarp to, kas turėjo būti gauta ir ką tinklas gauna. Sakysime, kad $f(h_n)$ bus funkcija $f$, kurios parametras $h_n$ yra paskutinio sluoksnio paslėptoji būsena.

Norint pakeisti $w_n$ ($n$-tojo elemento svorį), tai galima padaryti apskaičiuojant gradientą atsižvelgiant į $w_n$.

\begin{equation}
	\frac{\partial Loss}{\partial w_n} = \frac{\partial Loss}{\partial f(h_n)} \cdot \frac{\partial f(h_n)}{\partial w_n} = \frac{\partial Loss}{\partial f(h_n)} \cdot f'(h_n) \cdot w_n
\end{equation}

Čia $\frac{\partial Loss}{\partial f(h_n)}$ yra dalinė išvestinė to, kaip skaičiuojamas praradimas. Svarbu tai, kad jis skaičiuojamas iš $f(h_n)$, todėl tai bus pastovi grįžtamojo ryšio (\textit{angl. backpropagation}) lygtis.

Tęsiant toliau, pirmojo svorio $w_1$ reikšmę galima apskaičiuoti pagal šią lygtį:

\begin{equation}
\frac{\partial Loss}{\partial w_1} = \frac{\partial Loss}{\partial f(h_n)} \cdot \frac{\partial f(h_n)}{\partial h_{n-1}} \cdots \frac{\partial f(h_2)}{\partial h_1} \cdot \frac{\partial f(h_1)}{\partial w_1} = \frac{\partial f(Loss)}{\partial f(h_n)} \cdot f'(h_n) \cdot w_n \cdots f'(h_1) * w_1
\end{equation}

Dėl šios priežasties ilgainiui dėl per naują skaičiuojamų svorių, perceptronas susiduria su problema, kad „pamiršta“, kas buvo prieš daugiau nei vieną laiko momentą - $w_1$ palaipsniui pradeda nebekisti dėl ilgų skaičiavimų ir tampa labai mažas. Tai reiškia, kad rekurentiniai neuroniniai tinklai paprasčiausiai vadovaujasi trumpalaikės atminties principu. Dėl šios priežasties buvo sukurtos keletas architektūrų, kurios sugebėtų atsiminti ir teisingai įvertinti esamą situaciją. Toliau pateikiama dažniausiai šiuo metu naudojama RNN architektūra spręsti šią problemą.

\subsubsubsection{LSTM}
1997 metais Hochreiter ir Schmidhuber pristatė LSTM modelį, kuris, buvo manyta, galės išspręsti nykstančiųjų gradientų problemą. Šis modelis ypatingas tuo, jog kiekvienas įprastas paslėptojo sluoksnio mazags (\textit{angl. node}) yra pakeistas atminties ląstele \cite{DBLP:journals/corr/Lipton15}.

\textbf{LSTM} – ilga trumpalaikė atmintis (\textit{angl. long short-term memory}) - RNN architektūra, kuri sugeba atsiminti informaciją ilgesniam laiko tarpui. 

Paprasti RNN priima buvusią paslėptąją būseną, pritaiko aktyvacijos funkciją ir grąžina naują būseną. LSTM daro beveik tą patį, tik priima dar ir savo buvusią būseną ir grąžina savo naują būseną. 

LSTM įveda naują sąvoką - vartai (\textit{angl. gate}). LSTM turi trijų skirtingų tipų vartus:

\begin{itemize}
	\item \textbf{Užmaršties vartai} (\textit{angl. forget gate}) - juose apdorojama praeita paslėptoji būsena ir dabartinė įeiga. Šių vartų išeiga - nuosprendis, ką vertėtų pasilikti ląstelės būsenoje, o ką - užmiršti. Kuo vertė artimesnė 1 - tuo tai labiau verta atsiminti, o arčiau 0 - pamiršti;
	\item \textbf{Įeigos vartai} (\textit{angl. input gate}) - įeigos funkcija atnaujina ląstelės būseną;
	\item \textbf{Išeigos vartai} (\textit{angl. output gate}) nusprendžia, kurios ląstelės būsenos reikšmės bus pridedamas į paslėptąją būseną, kuri bus visos ląselės išeiga. Taip pat labai svarbu paminėti ir faktą, kad pasiliekamos ir tos reikšmės ar būsenos, kurios manoma, kad bus reikalingos ateityje.
\end{itemize}

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.12]{img/lstm}
	\caption{Atminties mazgo pavyzdys}
	\label{img:lstm}
\end{figure}


\textbf{Paprastas tinklas}
\begin{itemize}
	\item Du sluoksniai
	\item Pirmajame - atsisakymo lygis 0,8
	\item 128 - sluoksnių vienetų skaičius
\end{itemize}

\textbf{Gilus tinklas}
\begin{itemize}
	\item Trys sluoksniai
	\item Atsisakymo lygis 0,2 visuose lygiuose
	\item 64 - sluoksnių vienetų skaičius
\end{itemize}

\textbf{Platus tinklas}
\begin{itemize}
	\item Vienas sluoksnis
	\item Atsisakymo lygis 0,2
	\item 256 - sluoksnių vienetų skaičius
\end{itemize}

\textbf{Platesnis tinklas}
\begin{itemize}
	\item Vienas sluoksnis
	\item Atsisakymo lygis 0,2
	\item 512 - sluoksnių vienetų skaičius
\end{itemize}

\subsubsubsection{GRU}
2014 metais Cho pristatė GRU modelį, kuris, buvo manyta, galės išspręsti nykstančiųjų gradientų problemą \cite{DBLP:journals/corr/ChoMGBSB14}. Šis modelis yra panašus į LSTM ir abu modeliai dažnai duoda labai panašius rezultatus.

\textbf{GRU} – uždaromas pasikartojantis vienetas (\textit{angl. gated recurrent unit}) - RNN architektūra, kuri sugeba atsimintį informaciją ilgesniam laiko tarpui. 

Kaip ir LSTM taip ir GRU naudojasi vartais. Šiuo atveju yra dviejų tipų vartai:
\begin{itemize}
	\item \textbf{Atnaujinimo vartai} (\textit{angl. update gate}) - juose apdorojama nauja įeiga sudauginta su savo svoriu ir praėjusio laiko momento paslėptoji būsena su savo svoriu. Šie vartai nusprendžia, kiek aktuali praeituose laiko momentuose turima informacija ateityje;
	\item \textbf{Atstatymo vartai} (\textit{angl. reset gate}) - šie vartai savo veiksmu niekuo nesiskiria nuo atnaujinimo vartų, tačiau jie skirti tam, kad nustatytų, kurios kitų laiko momentų informacijos verta atsisakyti.
\end{itemize}

\subsubsubsection{LSTM ir GRU skirtumai}

LSTM ir GRU skiriasi tik skirtingais skaičiavimais neuronuose. Skiriasi vartų skaičius ir juose atliekami skaičiavimai. Tačiau rezultatas tarp LSTM ir GRU yra minimalus. Dažnai šie du modeliai grąžina arba labai panašius arba netgi identiškus rezultatus ir abu puikiai sprendžia nykstančiųjų gradientų problemą. Abu apdoroja praeituose laiko momentuose sukauptą informaciją, sprendžia, ką reikėtų pasilikti, ko reikėtų atsisakyti, priimami sprendimai remiantis prieš daugiau nei vieną laiko momentą priimti ar išmoktų sprendimų. 

\subsection{Apjungiamieji tinklų modeliai}

\textbf{Apjungiamieji tinklų modeliai} - modeliai, kuriuose yra apjungiami konvoliuciniai ir rekurentiniai neuroniniai tinklai. Pagal RNN specifikacijas to daryti neturėtų būti prasmės, tačiau pagal dabartines KNN ir RNN galimybes, KNN kur kas geriau atpažįsta tam tikras pasikartojančias savybes, o RNN - jų kitimą laike. Todėl tokie apjungiamieji tinklų modeliai dažniausiai naudojami atpažįstant šnekamąją kalbą. Taip pat tokie modeliai puikiausiai tinka apjungiant įeigos sekas ir išvedant statines išeigas. 

Yra du tipai apjungiamųjų tinklų modelių:
\begin{itemize}
	\item Viena įeiga - daug išeigų. Tokiu būdu iš vieno kadro RNN sugeba aprašyti kadrą pateikiant ne vieną tame kadre matomą objektą. Pavyzdžiui, pateikiant jūros su laivu vaizdą galima gauti aprašymą, kad matomas laivas, kuris plaukia jūra.
	\item Daug įeigų - viena išeiga. Tokiu būdų iš kadrų sekos RNN sugeba generuoti vieną išeigą. Kitaip tariant duodant, pavyzdžiui, video srautą, bus gaunama konkrečios klasės išeiga.
\end{itemize}

Galima pastebėti, kad būtent šiuo atveju, modelis, kuris apdoros video srautą ir nuspręs, kurios klasės įeiga buvo įeitimi yra labai naudingas. Yra žinoma, kad netgi gerai apmokius sistemą KNN ji iš kadro gali nuspręsti koks veiksmas atliekamas ar kokiai klasei yra priskiriamas kadras. Šiuo atveju norint apmokyti sistemą atpažinti gestų kalbą galima pasinaudoti KNN skirstyti gestus pakadriui ir tuomet juos apjungus RNN modeliu galima išvesti vieną bendrą klasę, kuri ir bus bendra viso vaizdo srauto klasė.


\section{Eksperimentinė dalis}

Šioje dalyje bus aprašomi visi atlikti eksperimentai ir juose gauti rezultatai.

\subsection{Panašūs darbai}
Dar prieš metus, sistemų, kurios atpažintų gestų kalbą konvoliucinių ar rekurentinių tinklų pagalba, beveik nebuvo. 2017 metais Harish Chandra Thuwal ir Adhyan Srivastava iš Jamia Millia Islamia universiteto Naujajame Delyje sukūrė konvoliucinių ir rekurentinių neuroninių tinklų modeliais paremtą sistemą, kuri sugeba atpažinti gestų kalbą iš video srauto. Šiame darbe jie vaizdo įrašą verčia į kadrų seką ir apmoko konvoliucinį tinklą. Vėliau iš šių duomenų apmoko rekurentinį neuroninį tinklą. Svarbu paminėti, kad šie du studentai pasinaudojo argentiniečių gestų kalbos duomenų rinkiniu, kuriame ant kiekvienos rankos žmonės, kurie rodė gestus, buvo užsidėję skirtingų spalvų pirštines. Taip jie iš vaizdo įrašo kadrų ištrindavo visą foną ir palikdavo tik rankas, taip apmokydami sistemą be papildomų trikdžių (\textit{angl. noise}).

\subsection{Argentiniečių gestų kalbos atpažinimas}

Buvo pasirinkta apmokyti jau esamą Harish Chandra Thuwal ir Adhyan Srivastava sukurtą modelį, jį tobulinant.

\subsubsection{Sistemos savybės}
\begin{itemize}
	\item KNN apmokymui pasirinktas kursiniame darbe jau išbandytas Inception v3 modelis
	\item kažkas?
\end{itemize}

\subsubsection{Bandymai}
\begin{table}[H]\footnotesize
	\centering
	\caption{Argentiniečių gestų kalbos bandymai su 3 klasėmis}
	{\begin{tabular}{| c | c | c | c | c | c | c |} \hline
		\thead{Bandymo\\Nr.} & \thead{Klasių\\skaičius} & \thead{Apmokymo\\tikslumas} & \thead{Epochų\\skaičius} & \thead{Tikslumas} & \thead{Praradimas} & \thead{Testavimas}  \\
		\hline
		1. & 3 & 100\% & 10 & 81.27\% & 0.6431 & 85.32\% \\
		\hline
		2. & 3 & 99.99\% & 100 & 89.27\% & 0.4422 & 93.33\% \\
		\hline
	\end{tabular}}
	\label{tab:asl-bandymai1}
\end{table}

Pats pirmasis bandymas buvo atliktas su trimis klasėmis, apmokant sistemą ir skaidant vaizdo įrašo kadrus kaip paveikslėlius. Kiekvienam iš jų buvo nuimamas fonas (\textit{angl. background}) ir paliekamos tik rankų plaštakos. Todėl buvo toks didelis apmokymo tikslumas.

Antruoju bandymu buvo atsisakyta nuimti foną ir palikti kadrus tokius, kokie yra. Dėl padidinto epochų skaičiaus rezultatai gauti geresni nei pirmuoju bandymu.

\begin{table}[H]\footnotesize
	\centering
	\caption{Argentiniečių gestų kalbos bandymai su 25 klasėmis}
	{\begin{tabular}{| c | c | c | c | c | c | c |} \hline
		\thead{Bandymo\\Nr.} & \thead{Klasių\\skaičius} & \thead{Apmokymo\\tikslumas} & \thead{Epochų\\skaičius} & \thead{RNN\\apmokymo\\tipas} & \thead{Tikslumas} & \thead{Praradimas}  \\
		\hline
		1. & 25 & 91.90\% & 100 & Platus & 91.99\% & 0.6839 \\
		\hline
		2. & 25 & 91.90\% & 100 & Platesnis & 91.95\% & 0.6255 \\
		\hline
		3. & 25 & 91.90\% & 100 & Gilus & 16.55\% & 2.0566 \\
		\hline
		4. & 25 & 91.90\% & 10 & Paprastas & 97.61\% & 0.2814 \\
		\hline
		5. & 25 & 91.90\% & 100 & Paprastas & 92.66\% & 0.5539 \\
		\hline
	\end{tabular}}
	\label{tab:asl-bandymai2}
\end{table}

\ref{tab:asl-bandymai2} lentelėje pateikiami dar 5 bandymai atlikti su argentiniečių gestų kalba. Šiuo atveju sistemos KNN buvo apmokytas vieną kartą, o toliau buvo keičiami RNN apmokymo būdai. Šie būdai buvo paremti LSTM modeliu. Galima pastebėti, kad giliuoju (\textit{angl. deep}) būdu rezultatai buvo prasčiausi. 

\subsection{Lietuvių gestų kalbos atpažinimas}

Nėra jokių oficialių duomenų lietuvių gestų kalbos atpažinimui naudojantis konvoliuciniais ar rekurentiniais neuroniniais tinklais. Šie bandymai, manoma, kad yra pirmieji naudojantis rekurentiniais neuroniniais tinklais atpažinti LGK. Konvoliucinių neuroninių tinklų pagalba tokį darbą dariau prieš metus, skirtą atpažinti statinę gestų kalbos abėcėlę pasinaudojant Inception v3 modeliu.

Lietuvių gestų kalbos atpažinimas iš video srauto galėtų palengvinti ne tik gestų kalbos nesuprantantiesiems bendrauti su pastaraisiais, bet ir, pavyzdžiui, turėti galimybę versti žodinę kalbą į gestų kalbą. 

Toliau pateikiamas visas darbas padarytas su lietuvių gestų kalbos atpažinimu, duomenų rinkimu ir gautais rezultatais.

\subsubsection{Duomenų paruošimas}
Oficialiame lietuvių gestų kalbos žodyne, kurį pristato \textbf{neįgaliųjų reikalų departamentas prie socialinės apsaugos ir darbo ministerijos}, pateikiama apie 9000 gestų. Žodynas rengiamas nuo 2004 metų kurčiųjų ir girdinčiųjų komandos. Šiame žodyne gestus galima rasti pagal žodį, gesto formą ar temą. Taip pat galima pasirinkti ar gesto ieškoti kaip atitinkamo žodžio ar naudojimo pavyzdžiuose. Susiradus tinkamą žodį yra aprašomos tokios specifikos kaip plaštakos forma, lūpų judesys, žodžio ar sakinio reikšmė. 

Naudojantis šiuo žodynu iškyla viena pagrindinė problema - kiekvienas gestas turi tik po vieną video įrašą atitinkantį tą žodį. Toks kiekis duomenų yra per mažas, norint apmokyti sistemą RNN būdu. Galima iš sakinių, kuriuose yra žodžio naudojimo pavyzdžiai, taip pat išskirti gestus, atitinkančius norimą gestą. Tačiau tai padidintų kiekvienos klasės duomenų kiekį iki daugiausiai 5 vaizdo įrašų. Net ir toks duomenų kiekis yra per mažas.

Nuspręsta duomenis susikurti. Teko pramokti lietuvių gestų kalbos gestus. Įsigilinti į gestų kalbos specifiką. Pirmiesiems bandymams buvo nufilmuota 3 skirtingų žodžių klasių gestai po 50 vaizdo įrašų kiekvienam, kas yra tapatu 150 video įrašų. Filmuota buvo mobiliuoju telefonu atsistojus prie gelsvos sienos. Filmuoti buvo du skirtingi asmenys, kurių kiekvienas atliko po 25 vaizdo įrašus kiekvienai klasei. Buvo pasirinkta pirmiesiems bandymams pasinaudoti „labas“, „mano“, „vardas“ žodžių klasėmis. Kiekvienas vaizdo įrašas truko ne ilgiau nei 3 sekundes.

Vėliau buvo nufilmuotos dar 22 klasės skirtingų gestų klasės. Tačiau šiuo atveju buvo nufilmuota po 20 kiekvienos klasės gestų, kuriuos atliko du žmonės, todėl kiekvienas iš jų atliko po 10 gesto pakartojimų kiekvienai klasei. Toliau padaryta duomenų augmentacija praplečiant kiekvienos klasės vaizdo įrašų kiekį iki 50 pritaikant kadrų modifikacijas - pasukant, išplečiant, susiaurinant ir kitaip keičiant kadrus.

\textbf{KELIOS FOTKĖS}

\textbf{Rezultatas} – duomenų bazė sudaryta iš 25 skirtingų lietuvių gestų kalbos klasių po 50 video įrašų, kas tapatu 1250 vaizdo įrašų. Kiekvieno video trukmė ne ilgesnė nei 4 sekundės, kas tapatu ne daugiau nei 120 kadrų\footnote{1 sekundė = 30 kadrų} kiekvienam įrašui.

\subsubsection{Modelio apmokymas}

Modelį buvo nuspręsta apmokyti pasinaudojant Harish Chandra Thuwal ir Adhyan Srivastava jau sukurtu modeliu jį patobulinus. Vienas iš svarbiausių pakeitimų - nenutrinti fono nuo kadrų. Taip pat pakeistas epochų skaičius, RNN LSTM tinklo apmokymo tipas, perdarytas testavimo mechanizmas.

Visų pirma buvo apmokyta sistema su trimis klasėmis „labas“, „mano“ ir „vardas“. Šie gestai visi atliekami dešiniąja ranka, todėl tai sistemai, buvo manyta, turėtų šiek tiek palengvinti darbą su duomenimis.

Pirmiausiai kiekvienas vaizdo įrašas buvo išskaidomas į kadrus. Dažniausiai gesto vaizdo įrašas truko apie 2 sekundes, bet ne ilgiau 3 sekundžių. Tai reiškia, kad filmuojant mobiliuoju įrenginiu pasirinkus 30 kadrų per sekundę būdą, kiekvienas gestas turėdavo apie 60 kadrų, daugiausiai 90 kadrų. Buvo nuspręsta, kad padaryti vienodus kiekius kadrų kiekvienam gestui. Tokiu atveju kadrų kiekis buvo pakeltas iki 120 kadrų kiekvienam vaizdo įrašui, kas lygu 4 sekundėms vaizdo įrašo. Kadangi visi įrašai skyrėsi savo ilgiu buvo nuspręsta, jei vaizdo įrašas per trumpas, paskutinį kadrą kartoti tiek kartų, kad visi įrašai turėtų vienodą kiekį kadrų. 

Buvo priimti tokie sprendimai, remiantis jau turimu argentiniečių gestų kalbos patobulintu apjungtu neuroniniu tinklu:
\begin{enumerate}
	\item Vaizdo įrašą skaidyti į 120 kadrų. Jei įrašas per trumpas - paskutinį kadrą kartoti $n$ kartų, kol kadrų bus 120;
	\item Konvoliucinį neuroninį tinklą apmokyti Inception v3 modeliu (\textit{žr. \ref{appendix:inception_v3} priede});
	\item RNN apmokyti pasirinkus paprastą modelį, kuris naudojasi LSTM architektūra;
	\item Mokymo - testavimo aibę skaidyti į 80\% mokymui ir 20\% testavimui, todėl mokymui buvo skirta 40 vaizdo įrašų, o testavimui - 10.
\end{enumerate}

\subsubsubsection{Pirmasis apmokymas}
HRLOE6
\textbf{Duomenys:}
\begin{itemize}
	\item 3 klasės - „labas“, „mano“, „vardas“;
	\item 40 vaizdo įrašai kiekvienai klasei. Viso - 120.
\end{itemize}

\textbf{Mokymas:}
\begin{itemize}
	\item KNN: 4000 žingsnių, galutinis tikslumas (final test accuracy) 98.9\%;
	\item Spėjimai iš KNN 14307 iš 14400 teisingi (99.35\%)
	\item RNN: 108 mokymo ir 12 pasitikrinimo vaizdo įrašų.
\end{itemize}

\textbf{DIAGRAMOS!!!!}

\textbf{Rezultatai:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: Tikslumas 86.93\% ir 0.5081 praradimas.
\end{itemize}



\subsubsubsection{Pirmasis 2 apmokymas}
PM1ITB
Vietoj kartojimo gale - kartojimas viduryje max 2 kartus
\textbf{Duomenys:}
\begin{itemize}
	\item 3 klasės - „labas“, „mano“, „vardas“;
	\item 40 vaizdo įrašai kiekvienai klasei. Viso - 120.
\end{itemize}

\textbf{Mokymas:}
\begin{itemize}
	\item KNN: 4000 žingsnių, galutinis tikslumas (final test accuracy) 99.0\%;
	\item Spėjimai iš KNN 14270 iš 14400 teisingi (99.10\%)
	\item RNN: 108 mokymo ir 12 pasitikrinimo vaizdo įrašų.
\end{itemize}

\textbf{DIAGRAMOS!!!!}

\textbf{Rezultatai:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: Tikslumas 86.93\% ir 0.5081 praradimas.
\end{itemize}


\subsubsubsection{Pirmasis 3 apmokymas}
Kartojimas viduryje 3 kartus
\textbf{Duomenys:}
\begin{itemize}
	\item 3 klasės - „labas“, „mano“, „vardas“;
	\item 40 vaizdo įrašai kiekvienai klasei. Viso - 120.
\end{itemize}

\textbf{Mokymas:}
\begin{itemize}
	\item KNN: 4000 žingsnių, galutinis tikslumas (final test accuracy) 99.1\%;
	\item Spėjimai iš KNN 14307 iš 144000
	\item RNN: 108 mokymo ir 12 pasitikrinimo vaizdo įrašų.
\end{itemize}

\textbf{DIAGRAMOS!!!!}

\textbf{Rezultatai:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: Tikslumas 86.93\% ir 0.5081 praradimas.
\end{itemize}

\subsubsubsection{Antrasis bandymas}
Antrajam bandymui su lietuvių gestų kalba buvo nufilmuota dar 22 papildomos LGK klasės. Kiekvienai klasei buvo nufilmuota po 20 video ir tuomet duomenys praplėsti iki 50 įrašų kiekvienai klasei pakeičiant įvairius parametrus tokius kaip ištempimas, sutraukimas, pasukimas. Plačiau - \textit{3.3.1. Duomenų paruošimas} skiltyje.

\textbf{Duomenys:}
KIR3RN
491NFW
HRQKWD
\begin{itemize}
	\item 25 klasės;
	\item 40 vaizdo įrašai kiekvienai klasei. Viso - 1000.
\end{itemize}

\textbf{Mokymas:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: 108 mokymo ir 12 pasitikrinimo vaizdo įrašų.
\end{itemize}

\textbf{DIAGRAMOS!!!!}

\textbf{Rezultatai:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: \textbf{REZULTATAI}.
\end{itemize}

\subsubsection{Trečiasis bandymas}

Trečiuoju bandymu buvo padaryta agresyvesnė duomenų augmentacija ir pasinaudota vos 10 nufilmuotų vaizdo įrašų. Tai reiškias, kad 80\% vaizdo įrašų buvo sugeneruota naudojantis OpenCV bibliotekos funkcijomis.

\textbf{Duomenys:}
\begin{itemize}
	\item 25 klasės;
	\item 40 vaizdo įrašai kiekvienai klasei. Viso - 1000.
\end{itemize}

\textbf{Mokymas:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: 108 mokymo ir 12 pasitikrinimo vaizdo įrašų.
\end{itemize}

\textbf{DIAGRAMOS!!!!}

\textbf{Rezultatai:}
\begin{itemize}
	\item KNN: \textbf{REZULTATAI};
	\item RNN: \textbf{REZULTATAI}.
\end{itemize}

\subsubsubsection{Apibendrinimas}
Toliau, \ref{tab:lgk-bandymai} lentelėje pateikiami visų trijų bandymų su lietuvių gestų kalba rezultatai.

\begin{table}[H]\footnotesize
	\centering
	\caption{Lietuvių gestų kalbos apmokymų rezultatai}
	{\begin{tabular}{| c | c | c | c | c | c | c |}
		\cline{4-7}
		\multicolumn{3}{ c |}{} & 
		\multicolumn{1}{ c |}{\thead{KNN}} &
		\multicolumn{3}{ c |}{\thead{RNN}} \\
		\hline
		\thead{Bandymo\\Nr.} & \thead{Klasių\\skaičius} & \thead{Vaizdo\\įrašų\\skaičius}  & \thead{Tikslumas} & \thead{Epochų\\skaičius}& \thead{Tikslumas} & \thead{Praradimas}  \\
		\hline
		1. & 3 & 120 & 98.9\% & 100 & 86.93\% & 0.5081 \\
		\hline
		2. & 25 & 1000 & . & . & .\% & . \\
		\hline
		3. & 25 & 1000 & . & . & .\% & . \\
		\hline
	\end{tabular}}
	\label{tab:lgk-bandymai}
\end{table}

Galima pastebėti, kad geriausi rezultatai buvo \textbf{X} bandymo, nes \textbf{...}, tačiau \textbf{...}

\subsubsection{Modelio testavimas}

Turimi visi trys modeliai buvo ištestuoti su 20\% visų duomenų. Toliau pateikiami visų trijų bandymų testavimai ir jų rezultatai.


\begin{table}[H]\footnotesize
	\centering
	\caption{Lietuvių gestų kalbos modelio testavimo rezultatai}
	{\begin{tabular}{| c | c | c | c |}
		\hline
		\thead{Bandymo\\Nr.} & \thead{Nematytų\\duomenų\\kiekis} & \thead{KNN\\pasirinkimas}  & \thead{RNN\\pasirinkimas}  \\
		\hline
		1. & 3 & 83.92\% & 79.31\% \\
		\hline
		1. & 10 & . & . \\
		\hline
		2. & 250 & . & . \\
		\hline
		3. & 250 & . & . \\
		\hline
	\end{tabular}}
	\label{tab:lgk-testavimas}
\end{table}

\ref{tab:lgk-testavimas} lentelėje pateikiami lietuvių gestų kalbos sukurto modelio testavimo rezultatai. Duomenys (vaizdo įrašai) duoti apmokymui ir testavimui beveik nesiskyrė, todėl buvo pabandyta ištestuoti ir su daugiau triukšmo turinčiais duomenimis, kurių sistema nėra mačiusi.

\begin{table}[H]\footnotesize
	\centering
	\caption{Lietuvių gestų kalbos modelio testavimo rezultatai su netvarkingais duomenimis}
	{\begin{tabular}{| c | c | c | c |}
		\hline
		\thead{Bandymo\\Nr.} & \thead{Nematytų\\duomenų\\kiekis} & \thead{TOP1\\pasirinkimas}  & \thead{TOP5\\pasirinkimas}  \\
		\hline
		1. & 6 & . & . \\
		\hline
		2. & 25 & . & . \\
		\hline
		3. & 25 & . & . \\
		\hline
	\end{tabular}}
	\label{tab:lgk-testavimas2}
\end{table}

\sectionnonum{Rezultatai ir išvados}
Rezultatų ir išvadų dalyje išdėstomi pagrindiniai darbo rezultatai (kažkas
išanalizuota, kažkas sukurta, kažkas įdiegta), toliau pateikiamos išvados
(daromi nagrinėtų problemų sprendimo metodų palyginimai, siūlomos
rekomendacijos, akcentuojamos naujovės). Rezultatai ir išvados pateikiami
sunumeruotų (gali būti hierarchiniai) sąrašų pavidalu. Darbo rezultatai turi
atitikti darbo tikslą.

\printbibliography[heading=bibintoc]  % Šaltinių sąraše nurodoma panaudota
% literatūra, kitokie šaltiniai. Abėcėlės tvarka išdėstomi darbe panaudotų
% (cituotų, perfrazuotų ar bent paminėtų) mokslo leidinių, kitokių publikacijų
% bibliografiniai aprašai. Šaltinių sąrašas spausdinamas iš naujo puslapio.
% Aprašai pateikiami netransliteruoti. Šaltinių sąraše negali būti tokių
% šaltinių, kurie nebuvo paminėti tekste. Šaltinių sąraše rekomenduojame
% necituoti savo kursinio darbo, nes tai nėra oficialus literatūros šaltinis.
% Jei tokių nuorodų reikia, pateikti jas tekste.

\sectionnonum{Sutartiniai žymėjimai}

\begin{itemize}
	\item $i_t$ – įeiga laiko momentu $t$
	\item $o_t$ – išeiga laiko momentu $t$
	\item $h_t$ – būsena laiko momentu $t$
\end{itemize}


\sectionnonum{Sąvokų apibrėžimai}

\begin{itemize}
	\item Dirbtiniai neuroniniai tinklai - artificial neural networks
	\item Inception v3 - Google modelis
	\item Išeiga - output
	\item Įeiga - input
	\item Konvoliuciniai neuroniniai tinklai - convolutional neural networks
	\item Neuroniniai tinklai - neural networks
	\item Paslėptasis sluoksnis - hidden layer
	\item Rekurentiniai neuroniniai tinklai - recurrent neural networks
	\item Sluoksnis - layer
	\item Vienpusiai neuroniniai tinklai - Feed-Forward neural networks
\end{itemize}

\sectionnonum{Santrumpos}
\begin{itemize}
	\item KNN - konvoliuciniai neuroniniai tinklai
	\item NN - neuroniniai tinklai
	\item RNN - Rekurentiniai neuroniniai tinklai
	
\end{itemize}

Sąvokų apibrėžimai ir santrumpų sąrašas sudaromas tada, kai darbo tekste
vartojami specialūs paaiškinimo reikalaujantys terminai ir rečiau sutinkamos
santrumpos.

\appendix  % Priedai
% Prieduose gali būti pateikiama pagalbinė, ypač darbo autoriaus savarankiškai
% parengta, medžiaga. Savarankiški priedai gali būti pateikiami ir
% kompaktiniame diske. Priedai taip pat numeruojami ir vadinami. Darbo tekstas
% su priedais susiejamas nuorodomis.

\section{Rankų pirštų numeracija}
\label{appendix:pirstai}
\begin{figure}[H]
    \centering
    \includegraphics[scale=1]{img/fingers}
    \caption{Kairės ir dešinės rankų pirštų numeracija}
    \label{img:fingers}
\end{figure}

\section{Konvoliucinio tinklo modelis}
\label{appendix:inception_v3}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.2]{img/inception_v3}
	\caption{Konvoliucinio tinklo modelis „Inception v3“}
	\label{img:inception_v3}
\end{figure}

%\section{Eksperimentinio palyginimo rezultatai}
%% tablesgenerator.com - converts calculators (e.g. excel) tables to LaTeX
%\begin{table}[H]\footnotesize
%  \centering
%  \caption{Lentelės pavyzdys}
%  {\begin{tabular}{|l|c|c|} \hline
%    Algoritmas & $\bar{x}$ & $\sigma^{2}$ \\
%    \hline
%    Algoritmas A  & 1.6335    & 0.5584       \\
%    Algoritmas B  & 1.7395    & 0.5647       \\
%    \hline
%  \end{tabular}}
%  \label{tab:table example}
%\end{table}

\end{document}
